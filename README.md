## Local Multi-Language Code RAG System

This project implements a local **RAG (Retrieval-Augmented Generation)** system designed to index and query a codebase. It allows you to ask questions about your code (Python, Java, JavaScript, Terraform, and GitLab CI files) and receive answers generated by a Large Language Model (LLM) based on your actual source files.

---

## Services and Architecture

The system is containerized using **Docker Compose**, orchestrating three primary services:

| Service | Tool | Purpose |
| --- | --- | --- |
| **Ollama** | `ollama/ollama` | Hosts and runs local LLMs (Mistral) and Embedding models (Nomic). |
| **Vector DB** | `chromadb/chroma` | An AI native open-source database used to store and search code embeddings. |
| **Backend** | `Python 3.11` | The "orchestrator" script (`one.py`) that processes files, manages embeddings, and handles queries. |

---

## Key Concepts: Understanding RAG

**Retrieval-Augmented Generation (RAG)** is a technique that gives an LLM access to specific, private data (like your code) without needing to retrain the model.

1. **Indexing (The "R" in RAG):**
* **Chunking:** Large code files are broken into smaller pieces (1500 characters each) to fit within the model's memory limits.
* **Embeddings:** Each chunk is converted into a numerical vector (a list of numbers) using the `nomic-embed-text` model. These numbers represent the *semantic meaning* of the code.
* **Vector Storage:** These vectors are stored in **ChromaDB**.


2. **Retrieval:** When you ask a question, the system converts your query into a vector and finds the 3 most similar code chunks in ChromaDB.
3. **Generation (The "G" in RAG):** The system sends your question *plus* those 3 code snippets to the **Mistral** LLM as "Context." This allows the AI to answer accurately based on your files.

---

## How to Run the Program

#### 1. Prerequisites

* Docker and Docker Compose installed.
* The `Modelfile` (referenced in compose) must exist in your directory.

#### 2. Start the Services

Run the following command to build and start the containers:

```bash
docker-compose up --build

```

This will automatically:

* Start Ollama and pull the `qwen2.5-coder:0.5b` model.
* Start the ChromaDB persistence server.
* Build the Python backend and install dependencies like `chromadb`, `ollama`, and `httpx`.

#### 3. Initialize Models

The system requires the `mistral` and `nomic-embed-text` models to be present in Ollama. You can use the provided script to ensure they are loaded:

```bash
bash loadmodel.sh

```

*(Note: Ensure `ollama pull nomic-embed-text` is also run if it is not in your script).*

---

## Project Structure & Files

* **`one.py`**: The core logic. It ignores standard directories like `.git` and `node_modules` while focusing on specific extensions (`.py`, `.js`, `.tf`, etc.).
* **`docker-compose.yml`**: Defines how the containers talk to each other. The backend uses the environment variables `CHROMA_HOST=chromadb` and `OLLAMA_BASE_URL` to communicate over the internal Docker network.
* **`Dockerfile`**: A slim Python environment that copies your local source code into the `/app` directory for indexing.

---

## Example Usage

Once running, the backend script will index its own directory by default. You can see the results of test queries in the logs, such as:

* "How is the system structured?"
* "What S3 bucket is defined?" (If Terraform files are present).

## `one.py` Technical Deep Dive

The `one.py` script serves as the **Orchestrator** for the RAG system. It handles the logic of reading code, converting it into a format the AI understands, and managing the conversation between the user and the LLM.

---

## Key Imports and Their Roles

* **`chromadb`**: The client used to connect to the vector database where code snippets and their "embeddings" (mathematical representations) are stored.
* **`ollama`**: The library used to communicate with the local Ollama server to generate embeddings and run the LLM (Mistral).
* **`os` & `sys**`: Used for navigating the file system and handling command-line arguments to choose which folder to index.

---

## Function-by-Function Explanation

The script is structured into three main phases: configuration, indexing, and querying.

#### 1. `get_chunks(text, size)`

* **Purpose**: Since LLMs have a "context window" (a limit on how much text they can process at once), this function breaks large source files into smaller, manageable pieces.
* **Mechanism**: It takes a string of code and returns a list of strings, each capped at the `CHUNK_SIZE` (default 1500 characters).

#### 2. `index_code(path_to_index)`

This is the most complex part of the script, responsible for building the knowledge base.

* **Filtering**: It uses `ignored_dirs` to skip heavy or irrelevant folders like `.git` or `node_modules`.
* **File Selection**: It targets specific extensions such as `.py`, `.java`, `.js`, and `.tf`. It specifically checks for GitLab CI files by looking for "gitlab-ci" in `.yml` filenames.
* **Embedding Generation**: For every chunk of code, it calls `ollama.embeddings(model=EMBED_MODEL, prompt=chunk)`. This turns the code into a vector.
* **Storage**: The code chunk, its vector, and metadata (file path and type) are saved into the **ChromaDB** collection.

#### 3. `query_code(user_query)`

This function performs the "Retrieval" and "Generation" steps of RAG.

* **Vector Search**: It converts the user's question into a vector and asks ChromaDB to find the top 3 most relevant code chunks.
* **Prompt Construction**: It wraps the retrieved code in an `[INST]` (instruction) tag, providing the snippets as "Context" to the LLM.
* **Final Answer**: It calls `ollama.generate` using the **Mistral** model to produce a natural language answer based strictly on the provided context.

---

## Execution Flow (The `if __name__ == "__main__":` block)

1. **Target Selection**: It looks at the first command-line argument to decide what to index; if none is provided, it indexes the current directory (`./`).
2. **Indexing**: Runs the `index_code` function for the target path.
3. **Validation**: If run in the standard way (no arguments), it executes a series of "Test Queries" to verify the system can see Python, Java, Javascript, and Terraform files.

In a RAG system, **chunk size** refers to the specific length of text segments that your document is divided into before being converted into mathematical vectors (embeddings) and stored in a database.

In your provided `one.py` script, this is controlled by the `CHUNK_SIZE = 1500` variable, which specifies that each piece of code will be roughly 1,500 characters long.

---

## How the Concept Works

The chunking process acts as a "sliding window" or a "fixed cut" that moves through your code files.

1. **Input:** The script reads a full file, such as a 5,000-character Python script.
2. **Splitting:** The `get_chunks` function slices that 5,000-character file into four distinct segments: three chunks of 1,500 characters and one final chunk of 500 characters.
3. **Embedding:** Each of these four chunks is sent individually to the embedding model (`nomic-embed-text`) to be turned into a vector.
4. **Retrieval:** When you ask a question, the system searches for the specific *chunk* that best matches your query, rather than the entire file.

---

## Why Chunk Size is Important

Choosing the right chunk size is a balancing act between **precision** and **context**.

#### 1. Context Window Limits

LLMs like Mistral and embedding models like Nomic have a "context window," which is the maximum amount of information they can process at once. If a chunk is too large, it may be truncated (cut off), causing the model to lose the end of the code snippet.

#### 2. The Precision vs. Context Trade-off

| Chunk Size | Pro: Precision | Con: Context |
| --- | --- | --- |
| **Small Chunks** | High precision. The system can find the exact line of code or specific variable you asked about. | Poor context. The AI might see a function call but not the actual function definition located a few lines above it. |
| **Large Chunks** | High context. The AI can see the entire logic of a class or a complex multi-step process. | Low precision. The "meaning" of the chunk becomes diluted because it covers too many different topics at once. |

#### 3. Avoiding "Information Noise"

If chunks are too large, the "noise" (irrelevant code) surrounding the relevant part can confuse the LLM, leading to hallucinations or incorrect answers. Conversely, if they are too small, the pieces are "orphaned" and lose their meaning.

---

## Optimization Tip: Adding "Overlap"

A common improvement to the logic in your `one.py` is adding a **chunk overlap**. This means if Chunk A ends at character 1500, Chunk B might start at character 1300. This 200-character overlap ensures that a function definition or a critical comment doesn't get cut in half between two chunks.


# Example Run 

## Docker run

```
docker compose run --rm backend python one.py
```

## Command  Local 

- > python one.py
```
--- Starting indexing for: ./ ---

Processing: ./one.py
Processing: ./samplefiles/hellol.js
Processing: ./samplefiles/terrademo.tf
Processing: ./samplefiles/App.java

Test Query: How is the system structured?

Result:  The system is structured as follows:


1. **Data Collection**: It indexes code from various sources (Python, Java, Javascript, GitLab CI files, Terraform files) by breaking them into smaller chunks, generating embeddings using Ollama's embedding model for each chunk, and storing the data in a ChromaDB collection.


2. **Querying**: To query the indexed data, it generates embeddings for user queries using the same Ollama model. It then uses these embeddings to search for the top 3 most relevant chunks in the ChromaDB collection.


3. **Response Generation**: The most relevant context found is then used as input for another LLM model (Mistral) to generate a response to the user query.


The system can be run either by providing a specific directory or file path as command-line arguments, or by running it without any arguments which will index all files in the current directory and allow you to test queries from the console.


Result (general):  The code provided is a Python script that uses Ollama, ChromaDB, and GitLab CI to index various code files (Python, Java, JavaScript, Terraform configurations, and GitLab CI YAML/YML files) and create a vector database for querying the contents.


To list all Python files in a given directory or folders, you can modify the `index_code` function to print the file names as follows:

```